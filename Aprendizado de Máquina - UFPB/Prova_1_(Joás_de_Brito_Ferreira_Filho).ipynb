{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prova 1 (Joás de Brito Ferreira Filho).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4KZCAgRVX_L"
      },
      "source": [
        "# 1ª Avaliação de Aprendizagem de Máquina - 12/11/2020\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iBPeBSVX_U"
      },
      "source": [
        "   * Nome: Joás de Brito Ferreira Filho\n",
        "   * Matrícula: 20200014569"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRhEWeVrVX_V"
      },
      "source": [
        "## Base de dados\n",
        "\n",
        "Acesse [https://tmfilho.github.io/akcdata/](https://tmfilho.github.io/akcdata/) e baixe a base de dados sobre as 277 raças de cachorros registradas no _American Kennel Club_. O conjunto contém 20 atributos de diferentes tipos, incluindo atributos textuais, numéricos e categóricos. Altura ( _Height_ ), peso ( _weight_ ) e expectativa de vida ( _life expectancy_ ) aparecem como intervalos \\[min, max\\]. A tabela abaixo é uma versão traduzida da tabela do site.\n",
        "\n",
        "\n",
        "|  #  | atributo | descrição | exemplo |\n",
        "| --- | ------ | ----------- | ------- |\n",
        "|  1  | description | 1 a 3 parágrafos descrevendo a raça | \"Akitas are burly, heavy-boned spitz-type dogs of imposing stature...\" |\n",
        "|  2  | temperament | temperamentoda raça descrito em palavras-chave | \"Confident, Famously Funny, Fearless\" |\n",
        "|  3  | popularity  | ranking de popularidade da raça (1-195) | 13 |\n",
        "|  4  | min_height  | altura mínima em cm | 60.96 |\n",
        "|  5  | max_height  | altura máxima em cm | 71.12 |\n",
        "|  6  | min_weight  | peso mínimo em kg | 3.18 |\n",
        "|  7  | max_weight  | peso máximo em kg | 4.54 |\n",
        "|  8  | min_expectancy  | expectativa de vida mínima em anos | 10 |\n",
        "|  9  | max_expectancy  | expectativa de vida máxima em anos | 12 |\n",
        "|  10  | group  | um de 9 grupos designados pelo AKC (7 grupos principais e 2 extras para raças novas ou que não se ajustam ainda a um grupo clássico) | \"Herding Group\" |\n",
        "|  11  | grooming_frequency_value  | Um número que representa a frequência de cuidados com o pelo | 0.4 |\n",
        "|  12  | grooming_frequency_category  | Categorização dos cuidados com o pelo | \"Weekly Brushing\" |\n",
        "|  13  | shedding_value  | Um número que representa a frequência de perda de pelo | 0.6 |\n",
        "|  14  | shedding_category  | Categorização da perda de pelos | \"Seasonal\" |\n",
        "|  15  | energy_level_value  | Um número que representa o nível de energia da raça | 0.8 |\n",
        "|  16  | energy_level_category  | Categorização do nível de energia | \"Energetic\" |\n",
        "|  17  | trainability_value  | Um número que representa a facilidade de treinar a raça | 1.0 |\n",
        "|  18  | trainability_category  | Uma categorização da facilidade de treinamento | \"Eager to Please\" |\n",
        "|  19  | demeanor_value  | Um número que representa a reação da raça a pessoas estranhas e outros animais | 0.6 |\n",
        "|  20  | demeanor_category  | Categorização da reação da raça a pessoas estranhas e outros animais | \"Alert/Responsive\" |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DykSdX_ovu_s",
        "outputId": "e6a2b151-5cac-4dde-cfc3-45a0763d7437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/tmfilho/akcdata/master/data/akc-data-latest.csv')\n",
        "\n",
        "dataset = data.dropna()\n",
        "\n",
        "data['group'].hist(figsize=[18, 10], xlabelsize=10,)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>min_height</th>\n",
              "      <th>max_height</th>\n",
              "      <th>min_weight</th>\n",
              "      <th>max_weight</th>\n",
              "      <th>min_expectancy</th>\n",
              "      <th>max_expectancy</th>\n",
              "      <th>grooming_frequency_value</th>\n",
              "      <th>shedding_value</th>\n",
              "      <th>energy_level_value</th>\n",
              "      <th>trainability_value</th>\n",
              "      <th>demeanor_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>min_height</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.935146</td>\n",
              "      <td>0.815263</td>\n",
              "      <td>0.792409</td>\n",
              "      <td>-0.460569</td>\n",
              "      <td>-0.463709</td>\n",
              "      <td>-0.186637</td>\n",
              "      <td>0.285123</td>\n",
              "      <td>0.174194</td>\n",
              "      <td>-0.049236</td>\n",
              "      <td>-0.239074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max_height</th>\n",
              "      <td>0.935146</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.793010</td>\n",
              "      <td>0.806405</td>\n",
              "      <td>-0.441227</td>\n",
              "      <td>-0.467058</td>\n",
              "      <td>-0.244199</td>\n",
              "      <td>0.320306</td>\n",
              "      <td>0.198560</td>\n",
              "      <td>-0.051842</td>\n",
              "      <td>-0.294494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min_weight</th>\n",
              "      <td>0.815263</td>\n",
              "      <td>0.793010</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.947519</td>\n",
              "      <td>-0.541745</td>\n",
              "      <td>-0.551445</td>\n",
              "      <td>-0.138691</td>\n",
              "      <td>0.284724</td>\n",
              "      <td>-0.072179</td>\n",
              "      <td>-0.132576</td>\n",
              "      <td>-0.276253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max_weight</th>\n",
              "      <td>0.792409</td>\n",
              "      <td>0.806405</td>\n",
              "      <td>0.947519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.513515</td>\n",
              "      <td>-0.522455</td>\n",
              "      <td>-0.116240</td>\n",
              "      <td>0.281833</td>\n",
              "      <td>-0.076534</td>\n",
              "      <td>-0.075793</td>\n",
              "      <td>-0.273650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min_expectancy</th>\n",
              "      <td>-0.460569</td>\n",
              "      <td>-0.441227</td>\n",
              "      <td>-0.541745</td>\n",
              "      <td>-0.513515</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.765434</td>\n",
              "      <td>0.028934</td>\n",
              "      <td>-0.251795</td>\n",
              "      <td>-0.000183</td>\n",
              "      <td>0.091815</td>\n",
              "      <td>0.181565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max_expectancy</th>\n",
              "      <td>-0.463709</td>\n",
              "      <td>-0.467058</td>\n",
              "      <td>-0.551445</td>\n",
              "      <td>-0.522455</td>\n",
              "      <td>0.765434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.127438</td>\n",
              "      <td>-0.356471</td>\n",
              "      <td>0.012556</td>\n",
              "      <td>0.094136</td>\n",
              "      <td>0.168534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grooming_frequency_value</th>\n",
              "      <td>-0.186637</td>\n",
              "      <td>-0.244199</td>\n",
              "      <td>-0.138691</td>\n",
              "      <td>-0.116240</td>\n",
              "      <td>0.028934</td>\n",
              "      <td>0.127438</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>-0.151441</td>\n",
              "      <td>0.098834</td>\n",
              "      <td>0.112278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shedding_value</th>\n",
              "      <td>0.285123</td>\n",
              "      <td>0.320306</td>\n",
              "      <td>0.284724</td>\n",
              "      <td>0.281833</td>\n",
              "      <td>-0.251795</td>\n",
              "      <td>-0.356471</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.173437</td>\n",
              "      <td>0.043577</td>\n",
              "      <td>-0.179746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_level_value</th>\n",
              "      <td>0.174194</td>\n",
              "      <td>0.198560</td>\n",
              "      <td>-0.072179</td>\n",
              "      <td>-0.076534</td>\n",
              "      <td>-0.000183</td>\n",
              "      <td>0.012556</td>\n",
              "      <td>-0.151441</td>\n",
              "      <td>0.173437</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.119501</td>\n",
              "      <td>-0.003133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trainability_value</th>\n",
              "      <td>-0.049236</td>\n",
              "      <td>-0.051842</td>\n",
              "      <td>-0.132576</td>\n",
              "      <td>-0.075793</td>\n",
              "      <td>0.091815</td>\n",
              "      <td>0.094136</td>\n",
              "      <td>0.098834</td>\n",
              "      <td>0.043577</td>\n",
              "      <td>0.119501</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.334465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>demeanor_value</th>\n",
              "      <td>-0.239074</td>\n",
              "      <td>-0.294494</td>\n",
              "      <td>-0.276253</td>\n",
              "      <td>-0.273650</td>\n",
              "      <td>0.181565</td>\n",
              "      <td>0.168534</td>\n",
              "      <td>0.112278</td>\n",
              "      <td>-0.179746</td>\n",
              "      <td>-0.003133</td>\n",
              "      <td>0.334465</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          min_height  ...  demeanor_value\n",
              "min_height                  1.000000  ...       -0.239074\n",
              "max_height                  0.935146  ...       -0.294494\n",
              "min_weight                  0.815263  ...       -0.276253\n",
              "max_weight                  0.792409  ...       -0.273650\n",
              "min_expectancy             -0.460569  ...        0.181565\n",
              "max_expectancy             -0.463709  ...        0.168534\n",
              "grooming_frequency_value   -0.186637  ...        0.112278\n",
              "shedding_value              0.285123  ...       -0.179746\n",
              "energy_level_value          0.174194  ...       -0.003133\n",
              "trainability_value         -0.049236  ...        0.334465\n",
              "demeanor_value             -0.239074  ...        1.000000\n",
              "\n",
              "[11 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sDE_p8mVX_X"
      },
      "source": [
        "**Questão 1 (1,0 ponto)** É possível extrair tarefas de classificação ou de regressão dessa base? Que variáveis poderiam ser alvos nessas tarefas e quais seriam os atributos/variáveis independentes? **Ainda não vimos processamento de linguagem, então podem desconsiderar a variável description**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSbnI2CCVX_Y"
      },
      "source": [
        "# possible targets: \n",
        "#   popularity (predictable by regression)\n",
        "#   group (predictable by classification)\n",
        "\n",
        "# Pode-se, a partir dos dados apresentados, construir tanto um modelo de \n",
        "# classificação quanto um modelo de regressão, a depender das variáveis alvo, \n",
        "# que podem ser popularity ou group, respectivamente, modeláveis por regressão \n",
        "# ou por classificação.\n",
        "\n",
        "# O problema de que combinação de variáveis independentes determina o modelo \n",
        "# mais eficiente é melhor resolvido após experimentação, de modo que, com o \n",
        "# tratamento adequado dos dados, todas as outras variáveis no dataset podem vir \n",
        "# a ser atributos de um modelo de regressão ou de classificação."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgy82SNhVX_g"
      },
      "source": [
        "**Questão 2 (1,0 ponto)** Como você trataria as variáveis categóricas nessas tarefas? Faça esse tratamento e mostre a DataFrame resultante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1AyOkuIVX_h",
        "outputId": "055821c5-903e-4c89-8235-93712a6d2cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "# one-hot encoding\n",
        "from pandas import get_dummies\n",
        "\n",
        "categorical_columns = ['grooming_frequency_category', \n",
        "                       'shedding_category', 'energy_level_category',\n",
        "                       'trainability_category', 'demeanor_category']\n",
        "categorical_data = dataset[categorical_columns]\n",
        "\n",
        "dummies = get_dummies(categorical_data)\n",
        "dummies\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grooming_frequency_category_2-3 Times a Week Brushing</th>\n",
              "      <th>grooming_frequency_category_Daily Brushing</th>\n",
              "      <th>grooming_frequency_category_Occasional Bath/Brush</th>\n",
              "      <th>grooming_frequency_category_Specialty/Professional</th>\n",
              "      <th>grooming_frequency_category_Weekly Brushing</th>\n",
              "      <th>shedding_category_Frequent</th>\n",
              "      <th>shedding_category_Infrequent</th>\n",
              "      <th>shedding_category_Occasional</th>\n",
              "      <th>shedding_category_Regularly</th>\n",
              "      <th>shedding_category_Seasonal</th>\n",
              "      <th>energy_level_category_Calm</th>\n",
              "      <th>energy_level_category_Couch Potato</th>\n",
              "      <th>energy_level_category_Energetic</th>\n",
              "      <th>energy_level_category_Needs Lots of Activity</th>\n",
              "      <th>energy_level_category_Regular Exercise</th>\n",
              "      <th>trainability_category_Agreeable</th>\n",
              "      <th>trainability_category_Eager to Please</th>\n",
              "      <th>trainability_category_Easy Training</th>\n",
              "      <th>trainability_category_Independent</th>\n",
              "      <th>trainability_category_May be Stubborn</th>\n",
              "      <th>demeanor_category_Alert/Responsive</th>\n",
              "      <th>demeanor_category_Aloof/Wary</th>\n",
              "      <th>demeanor_category_Friendly</th>\n",
              "      <th>demeanor_category_Outgoing</th>\n",
              "      <th>demeanor_category_Reserved with Strangers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     grooming_frequency_category_2-3 Times a Week Brushing  ...  demeanor_category_Reserved with Strangers\n",
              "0                                                    1      ...                                          0\n",
              "1                                                    0      ...                                          0\n",
              "2                                                    1      ...                                          0\n",
              "3                                                    0      ...                                          0\n",
              "4                                                    1      ...                                          0\n",
              "..                                                 ...      ...                                        ...\n",
              "270                                                  0      ...                                          0\n",
              "271                                                  0      ...                                          0\n",
              "272                                                  0      ...                                          0\n",
              "274                                                  0      ...                                          0\n",
              "276                                                  0      ...                                          0\n",
              "\n",
              "[188 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaclFXYtVX_m"
      },
      "source": [
        "**Questão 3 (1,0 ponto)** As variáveis tem escalas diferentes. Como você trataria essa situação para não prejudicar o desempenho de modelos que são sensíveis a isso? Faça esse tratamento e mostre a DataFrame resultante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-m6TYCVX_n",
        "outputId": "6f5a888a-51dc-40e3-b786-8aef7d7f7117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "# normalização\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "numerical_columns = []\n",
        "for column in dataset.columns:\n",
        "  if (dataset[column].dtype == 'float64'):\n",
        "    numerical_columns.append(column)\n",
        "numerical_data = dataset[numerical_columns]\n",
        "\n",
        "normalized_data = normalize(numerical_data)\n",
        "normalized_data = pd.DataFrame(normalized_data)\n",
        "normalized_data.columns = numerical_columns\n",
        "normalized_data\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>min_height</th>\n",
              "      <th>max_height</th>\n",
              "      <th>min_weight</th>\n",
              "      <th>max_weight</th>\n",
              "      <th>min_expectancy</th>\n",
              "      <th>max_expectancy</th>\n",
              "      <th>grooming_frequency_value</th>\n",
              "      <th>shedding_value</th>\n",
              "      <th>energy_level_value</th>\n",
              "      <th>trainability_value</th>\n",
              "      <th>demeanor_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.542111</td>\n",
              "      <td>0.692698</td>\n",
              "      <td>0.075297</td>\n",
              "      <td>0.107567</td>\n",
              "      <td>0.284573</td>\n",
              "      <td>0.355716</td>\n",
              "      <td>0.014229</td>\n",
              "      <td>0.014229</td>\n",
              "      <td>0.014229</td>\n",
              "      <td>0.018972</td>\n",
              "      <td>0.023714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.623842</td>\n",
              "      <td>0.673749</td>\n",
              "      <td>0.222811</td>\n",
              "      <td>0.267373</td>\n",
              "      <td>0.117891</td>\n",
              "      <td>0.147364</td>\n",
              "      <td>0.007859</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.007859</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.001965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.627489</td>\n",
              "      <td>0.627489</td>\n",
              "      <td>0.243602</td>\n",
              "      <td>0.341042</td>\n",
              "      <td>0.118151</td>\n",
              "      <td>0.150374</td>\n",
              "      <td>0.006445</td>\n",
              "      <td>0.004296</td>\n",
              "      <td>0.006445</td>\n",
              "      <td>0.010741</td>\n",
              "      <td>0.008593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.524050</td>\n",
              "      <td>0.611392</td>\n",
              "      <td>0.272955</td>\n",
              "      <td>0.506917</td>\n",
              "      <td>0.085966</td>\n",
              "      <td>0.111756</td>\n",
              "      <td>0.006877</td>\n",
              "      <td>0.005158</td>\n",
              "      <td>0.006877</td>\n",
              "      <td>0.008597</td>\n",
              "      <td>0.005158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.573210</td>\n",
              "      <td>0.623054</td>\n",
              "      <td>0.333794</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.098119</td>\n",
              "      <td>0.137366</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>0.007849</td>\n",
              "      <td>0.003925</td>\n",
              "      <td>0.007849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>0.654715</td>\n",
              "      <td>0.654715</td>\n",
              "      <td>0.116919</td>\n",
              "      <td>0.140303</td>\n",
              "      <td>0.206209</td>\n",
              "      <td>0.257762</td>\n",
              "      <td>0.006874</td>\n",
              "      <td>0.003437</td>\n",
              "      <td>0.010310</td>\n",
              "      <td>0.010310</td>\n",
              "      <td>0.010310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0.570527</td>\n",
              "      <td>0.684633</td>\n",
              "      <td>0.178298</td>\n",
              "      <td>0.356596</td>\n",
              "      <td>0.134770</td>\n",
              "      <td>0.168463</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>0.006739</td>\n",
              "      <td>0.011231</td>\n",
              "      <td>0.006739</td>\n",
              "      <td>0.006739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>0.587436</td>\n",
              "      <td>0.683065</td>\n",
              "      <td>0.219567</td>\n",
              "      <td>0.317152</td>\n",
              "      <td>0.129083</td>\n",
              "      <td>0.150597</td>\n",
              "      <td>0.002151</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.008606</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.006454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>0.352389</td>\n",
              "      <td>0.810494</td>\n",
              "      <td>0.062929</td>\n",
              "      <td>0.346112</td>\n",
              "      <td>0.180356</td>\n",
              "      <td>0.249724</td>\n",
              "      <td>0.002775</td>\n",
              "      <td>0.002775</td>\n",
              "      <td>0.011099</td>\n",
              "      <td>0.008324</td>\n",
              "      <td>0.008324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>0.536753</td>\n",
              "      <td>0.613432</td>\n",
              "      <td>0.095853</td>\n",
              "      <td>0.095853</td>\n",
              "      <td>0.332074</td>\n",
              "      <td>0.452828</td>\n",
              "      <td>0.030189</td>\n",
              "      <td>0.006038</td>\n",
              "      <td>0.018113</td>\n",
              "      <td>0.006038</td>\n",
              "      <td>0.024151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     min_height  max_height  ...  trainability_value  demeanor_value\n",
              "0      0.542111    0.692698  ...            0.018972        0.023714\n",
              "1      0.623842    0.673749  ...            0.001965        0.001965\n",
              "2      0.627489    0.627489  ...            0.010741        0.008593\n",
              "3      0.524050    0.611392  ...            0.008597        0.005158\n",
              "4      0.573210    0.623054  ...            0.003925        0.007849\n",
              "..          ...         ...  ...                 ...             ...\n",
              "183    0.654715    0.654715  ...            0.010310        0.010310\n",
              "184    0.570527    0.684633  ...            0.006739        0.006739\n",
              "185    0.587436    0.683065  ...            0.006454        0.006454\n",
              "186    0.352389    0.810494  ...            0.008324        0.008324\n",
              "187    0.536753    0.613432  ...            0.006038        0.024151\n",
              "\n",
              "[188 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mql13eN2VX_s"
      },
      "source": [
        "**Questão 4 (1,5 ponto)** Suponha uma tarefa que envolva predizer a popularidade de uma raça. Essa é uma tarefa de regressão ou de classificação? Selecione atributos para usar como variáveis independentes nessa tarefa. Faça uma análise de importância das variáveis realizando testes de coeficientes do ajuste linear e usando os valores de importância dados por um modelo de árvore de decisão. Note que alguns valores podem estar faltando para certas raças. **Pode usar bibliotecas relevantes. Não é necessário implementar do zero**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoCOAZhgVX_t",
        "outputId": "9a51241f-efe4-4513-8886-b776b9444471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from numpy import hstack, delete\n",
        "\n",
        "# we have previously preprocessed the data (normalizing the numerical columns \n",
        "# and making one-hot encoding in categorical columns) so we just need to concatenate\n",
        "# this data into X\n",
        "X = hstack([dummies, normalized_data])\n",
        "\n",
        "# in order to achieve a successful prediction we need to preprocess the target variable\n",
        "modes = dataset['popularity'].mode()\n",
        "dataset['popularity'].iloc[14] = modes[0]\n",
        "dataset['popularity'].iloc[15] = modes[1]\n",
        "y = normalize([pd.to_numeric(dataset['popularity'])])[0]\n",
        "\n",
        "# to evaluate each coefficient's significance we can use the entire dataset to\n",
        "# gain a wider insight about the relationship between the target and the features\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X, y)\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# now we can enumerate these importances to have a notion about it\n",
        "for i, j in enumerate(importances):\n",
        "  print(\"Feature: \", i, \" Importance: \", j)\n",
        "\n",
        "# we can easily see that the features 3, 6, 10, 11 and 21 have importance 0\n",
        "# in the prediction task, so we discard these columns from our feature vector X\n",
        "X = delete(X, [1, 3, 5, 11, 23], axis=1)\n",
        "\n",
        "# now we rebuild our model using the selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# next we're gonna evaluate our model\n",
        "kfolds = KFold(n_splits=8, random_state=17, shuffle=True)\n",
        "validation = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature:  0  Importance:  0.014448979420258279\n",
            "Feature:  1  Importance:  1.4618249600442189e-05\n",
            "Feature:  2  Importance:  0.0006611946742355088\n",
            "Feature:  3  Importance:  0.0\n",
            "Feature:  4  Importance:  0.0\n",
            "Feature:  5  Importance:  4.132466713971983e-05\n",
            "Feature:  6  Importance:  8.433605539241909e-07\n",
            "Feature:  7  Importance:  1.686721107748518e-06\n",
            "Feature:  8  Importance:  0.044581450335259655\n",
            "Feature:  9  Importance:  0.0012258245650527137\n",
            "Feature:  10  Importance:  0.0\n",
            "Feature:  11  Importance:  0.0\n",
            "Feature:  12  Importance:  0.0022989607098279825\n",
            "Feature:  13  Importance:  0.0\n",
            "Feature:  14  Importance:  0.00093149173175144\n",
            "Feature:  15  Importance:  2.4457456062288167e-05\n",
            "Feature:  16  Importance:  0.07297176567858614\n",
            "Feature:  17  Importance:  7.590244984844791e-06\n",
            "Feature:  18  Importance:  0.0010563090937244846\n",
            "Feature:  19  Importance:  0.0\n",
            "Feature:  20  Importance:  0.023542757914893873\n",
            "Feature:  21  Importance:  0.0001652986685588772\n",
            "Feature:  22  Importance:  0.003978272292705765\n",
            "Feature:  23  Importance:  3.036097993942646e-05\n",
            "Feature:  24  Importance:  0.028072165112584847\n",
            "Feature:  25  Importance:  0.05756697875955894\n",
            "Feature:  26  Importance:  0.024866260400657723\n",
            "Feature:  27  Importance:  0.026588344595766172\n",
            "Feature:  28  Importance:  0.1802419037662285\n",
            "Feature:  29  Importance:  0.061845719800572914\n",
            "Feature:  30  Importance:  0.17909162835582756\n",
            "Feature:  31  Importance:  0.05788858514837628\n",
            "Feature:  32  Importance:  0.08319006379951913\n",
            "Feature:  33  Importance:  0.022420548088084656\n",
            "Feature:  34  Importance:  0.019728836514124836\n",
            "Feature:  35  Importance:  0.09251577889445521\n",
            "MEAN SQUARED ERROR:  -0.001321736034241426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rt4NeSfVX_1"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# ENUMERAÇÃO DOS MODELOS ESTUDADOS:\n",
        "#-------------------------------------------------------------------------------\n",
        "#   Bayes Ótimo\n",
        "#       Utiliza de manipulações sobre o Teorema de Bayes para chegar a\n",
        "#       uma estimativa de Yi dadas as variáveis independentes Xi.\n",
        "#       \n",
        "#   Modelos Lineares\n",
        "#       Assume uma relação linear entre as entradas Xi e a variável \n",
        "#       alvo Yi, de modo que um modelo linear pode ser representado por\n",
        "#       coeficientes reais Bi (vetor beta), sendo Yi = Sum((Xi)(Bi))\n",
        "#\n",
        "#   K-Vizinhos mais Próximos\n",
        "#       Considera que observações Xi mais próximas são mais semelhantes\n",
        "#       do que observações mais distantes entre si, determinando \n",
        "#       vizinhanças que delimitam regiões Yi, com base nas K observações\n",
        "#       mais próximas de cada observação Xi\n",
        "#\n",
        "#   Árvores de Decisão\n",
        "#       Estabelece as classes Yi por meio da discriminação das \n",
        "#       observações Xi, ramificando cada possível valor de Yi\n",
        "#       em condições sobre X\n",
        "#\n",
        "#   Máquinas de Vetor Suporte\n",
        "#       Modelos que dividem as classes Yi utilizando hiperplanos, sejam\n",
        "#       lineares ou não, e sendo ajustável por meio da função de kernel\n",
        "#\n",
        "#   Redes Neurais\n",
        "#       Baseadas em camadas de neurônios artificiais, Redes Neurais \n",
        "#       Artificiais modelam cada um de seus neurônios como uma função \n",
        "#       de ativação alimentada por uma combinação linear do vetor de \n",
        "#       entradas X, onde cada camada funciona como a entrada da \n",
        "#       camada seguinte\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Métricas de Avaliação de Classificadores\n",
        "#-------------------------------------------------------------------------------\n",
        "#   Matriz de Confusão"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cejKR05OVX_0"
      },
      "source": [
        "**Questão 5 (2,0 pontos)** Faça uma avaliação de desempenho dos modelos que vimos até agora no curso, considerando a tarefa de predizer a popularidade da raça e usando validação cruzada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nftf7RrEO8VT"
      },
      "source": [
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "\n",
        "# target and attributes\n",
        "y = normalize([pd.to_numeric(dataset['popularity'])])[0]\n",
        "X = hstack([normalized_data, dummies])\n",
        "# train and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT7ub5GkEp-y",
        "outputId": "d08a5a56-3368-4efa-e45e-cb308e2798ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "validation = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN SQUARED ERROR:  -0.0013430994678631009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYxlwmhSddk",
        "outputId": "ea5296f7-dc95-4d59-8df4-9a16d55b6e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Ridge Regression\n",
        "\n",
        "model = Ridge().fit(X_train, y_train)\n",
        "validation_ridge = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN SQUARED ERROR:  -0.0013449931434107503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s38nKoP0OfHJ",
        "outputId": "387d0418-beca-47a3-8703-49cb1e97b4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lasso Regression\n",
        "\n",
        "model = Lasso().fit(X_train, y_train)\n",
        "validation_lasso = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN SQUARED ERROR:  -0.0013711294707017673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmxTiiSVS5GH",
        "outputId": "b3089a47-c18c-40fe-fbdf-5468b2647227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Elastic Net Regression\n",
        "\n",
        "model = ElasticNet().fit(X_train, y_train)\n",
        "validation_elasticnet = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN SQUARED ERROR:  -0.0013711294707017673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3We0mMJ9XIch",
        "outputId": "ae25bfdf-de52-44a0-dc74-be5e8c136c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Neural Network\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "model = MLPRegressor().fit(X_train, y_train)\n",
        "validation_neuralnetwork = cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
        "print(\"MEAN SQUARED ERROR: \", validation.mean())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN SQUARED ERROR:  -0.01300085525892227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkImyjhBYwUF"
      },
      "source": [
        "# Como podemos ver, o modelo que se saiu pior foi o Multilayer Perceptron,\n",
        "# ainda assim todos os modelos estão na mesma faixa de erro, isto é, o RMSE é \n",
        "# aproximadamente igual a 0.013\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiHNwsrcVX_6"
      },
      "source": [
        "**Questão 6 (1,5 pontos)** Suponha uma tarefa que envolva predizer o grupo de uma raça. Essa é uma tarefa de regressão ou de classificação? Selecione variáveis para usar como atributos nessa tarefa. Faça uma análise de importância das variáveis realizando testes de coeficientes do ajuste linear e usando os valores de importância dados por um modelo de árvore de decisão. **Pode usar bibliotecas relevantes. Não é necessário implementar do zero**. Nota: as classes _Foundation Stock Service_ e _Miscellaneous Class_ são classes não-descritivas e podem ser desconsideradas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWa0WuTNVX_7",
        "outputId": "6b89b577-4e19-455a-82a6-b35f363921d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Gaussian Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from numpy import hstack\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from numpy import array\n",
        "\n",
        "# target and attributes of the task\n",
        "y = dataset['group']\n",
        "X = pd.DataFrame(hstack([normalized_data, dummies]))\n",
        "\n",
        "# feature selection by decision tree importances\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X, y)\n",
        "importances = model.feature_importances_\n",
        "print(importances)\n",
        "X.drop([12, 14, 16, 17, 20, 21, 22,28, 29, 30, 31, 32, 34], axis=1, inplace=True)\n",
        "\n",
        "# training and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
        "\n",
        "# the model\n",
        "GaussianNaiveBayesClassifier = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "# model evaluation\n",
        "preds = GaussianNaiveBayesClassifier.predict(X_test)\n",
        "print(\"Gaussian Naive Bayes Classifier Evaluation:\\n\")\n",
        "print(\"\\tScore: \", GaussianNaiveBayesClassifier.score(X, y)*100, \"%\")\n",
        "print(\"\\tAccuracy: \", accuracy_score(y_test, preds)*100, \"%\")\n",
        "\n",
        "# WITH THE WHOLE DATASET\n",
        "#   Score:  0.4095744680851064\n",
        "#   Accuracy:  0.23404255319148937\n",
        "\n",
        "# WITH SELECTED FEATURES\n",
        "# Score:  50.53191489361703 %\n",
        "# Accuracy:  31.914893617021278 %\n",
        "\n",
        "\n",
        "# even though our model didn't performed very well, we can see that a subtle\n",
        "# adjustment on the inputs (we deleted the features where importance=0) \n",
        "# increased the metrics we used to evaluate the model\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06480895 0.02848809 0.00415451 0.16832701 0.04618928 0.12735114\n",
            " 0.06566224 0.08202725 0.05966748 0.09286502 0.08921494 0.01931049\n",
            " 0.         0.00623177 0.         0.         0.         0.01454079\n",
            " 0.01626734 0.0149929  0.         0.         0.         0.02624762\n",
            " 0.00623177 0.00934765 0.00997083 0.01968052 0.         0.\n",
            " 0.         0.         0.         0.01723438 0.         0.01118803]\n",
            "Gaussian Naive Bayes Classifier Evaluation:\n",
            "\n",
            "\tScore:  50.53191489361703 %\n",
            "\tAccuracy:  31.914893617021278 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RpIxXvnVYAF"
      },
      "source": [
        "**Questão 7 (2,0 ponto)** Faça uma avaliação de desempenho dos modelos que vimos até agora no curso, considerando a tarefa de predizer o grupo da raça e usando validação cruzada. Avalie acurácia e precisão e cobertura por classe. Mostre a matriz de confusão do melhor de todos os modelos ajustados durante a validação cruzada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT-QATowozFd",
        "outputId": "83506e6a-f902-42c9-fee8-e72ca04878b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, preds)*100, \"%\")\n",
        "print(\"Precision: \", precision_score(y_test, preds, average='macro'))\n",
        "for i, p in enumerate(precision_score(y_test, preds, average=None)):\n",
        "  print(\"Precision \", i, \" =\", p*100, \"%\")"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  31.914893617021278 %\n",
            "Precision:  0.4209183673469387\n",
            "Precision  0  = 25.0 %\n",
            "Precision  1  = 12.5 %\n",
            "Precision  2  = 0.0 %\n",
            "Precision  3  = 0.0 %\n",
            "Precision  4  = 57.14285714285714 %\n",
            "Precision  5  = 100.0 %\n",
            "Precision  6  = 100.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6FYlZbPziws",
        "outputId": "0420dae0-5d61-4cf7-8d90-d580479f55f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "# Linear Regression Classifier\n",
        "\n",
        "# in order to perform a classification task by using linear regression\n",
        "# we need to fit a model for each label\n",
        "y_dummies = get_dummies(y)\n",
        "for y in y_dummies:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
        "  model = LinearRegression().fit(X_train, y_train)\n",
        "  preds = model.predict(X_test)\n",
        "  print(\"Model \", i, \"\\tScore: \", model.score(X, y)*100, \"%\")\n",
        "  print(\"Model \", i, \"\\tAccuracy: \", accuracy_score(y_test, preds)*100, \"%\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-2fc7431e92b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_dummies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2116\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [188, 13]"
          ]
        }
      ]
    }
  ]
}